{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed81de81-f00b-4201-8953-f55f0efb3f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.relpath(\"../src/\"))\n",
    "from dataloader import S1, S2, S3, S4\n",
    "import emg\n",
    "from utils import create_sliding_window_features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6523aa2-dcd2-4dfb-9e6a-01cab4362ff1",
   "metadata": {},
   "source": [
    "1. create sliding window, the input size is num_signals * window_size * 1\n",
    "2. split train (80%) and test (20%) set on the provided dataset for different scenarios.\n",
    "   For one scenario, we have one training set and multiple test sets, and the training data from each sub-dataset are concatenated into one file.\n",
    "3. use tabular predictor first\n",
    "4. next, explore how to analyze time series data by autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9269c87a-adcf-454d-8553-703636d136fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_prefix = '../data'\n",
    "save_prefix = '../splitted_data/'\n",
    "features = ['ecg', 'bvp', 'gsr', 'rsp', 'emg_zygo', 'emg_coru', 'emg_trap']\n",
    "s1 = S1()\n",
    "\n",
    "train_pairs = s1.train_test_indices['train']\n",
    "\n",
    "train_data = []\n",
    "for sub, vid in train_pairs:\n",
    "    physiology, annotations = s1.train_data(sub, vid, features=features)\n",
    "    X, y = create_sliding_window_features(physiology, annotations, window_size=50)\n",
    "    \n",
    "    df = X.join(y)\n",
    "    \n",
    "    length = len(df)\n",
    "    \n",
    "    train_length = int(length * 0.8)\n",
    "    test_length = length - train_length\n",
    "\n",
    "    train_data.append(df[:train_length])\n",
    "    df[train_length:].to_csv(os.path.join(save_prefix, 'scenario_1/test', f'sub_{sub}_vid_{vid}.csv'), index_label='time')\n",
    "\n",
    "train_data = pd.concat(train_data, axis=0)\n",
    "train_data.to_csv(os.path.join(save_prefix, 'scenario_1/train', 'train.csv'), index_label='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdea25cf-37c8-4d53-90a0-381abce586eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_prefix = '../data'\n",
    "save_prefix = '../splitted_data/'\n",
    "features = ['ecg', 'bvp', 'gsr', 'rsp', 'emg_zygo', 'emg_coru', 'emg_trap']\n",
    "s2 = S2()\n",
    "\n",
    "folds = s2.train_test_indices\n",
    "for fold, train_test_pairs in enumerate(folds):\n",
    "    train_pairs = train_test_pairs['train']\n",
    "\n",
    "    train_data = []\n",
    "    for sub, vid in train_pairs:\n",
    "        physiology, annotations = s2.train_data(fold, sub, vid, features=features)\n",
    "        X, y = create_sliding_window_features(physiology, annotations, window_size=50)\n",
    "\n",
    "        df = X.join(y)\n",
    "\n",
    "        length = len(df)\n",
    "\n",
    "        train_length = int(length * 0.8)\n",
    "        test_length = length - train_length\n",
    "\n",
    "        train_data.append(df[:train_length])\n",
    "        df[train_length:].to_csv(os.path.join(save_prefix, f'scenario_2/test/fold_{fold}', f'sub_{sub}_vid_{vid}.csv'), index_label='time')\n",
    "\n",
    "    train_data = pd.concat(train_data, axis=0)\n",
    "    train_data.to_csv(os.path.join(save_prefix, f'scenario_2/train/fold_{fold}', 'train.csv'), index_label='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6e62bf-702b-4a42-9ba5-ed5eaae5ba3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_prefix = '../data'\n",
    "save_prefix = '../splitted_data/'\n",
    "features = ['ecg', 'bvp', 'gsr', 'rsp', 'emg_zygo', 'emg_coru', 'emg_trap']\n",
    "s3 = S3()\n",
    "\n",
    "folds = s3.train_test_indices\n",
    "for fold, train_test_pairs in enumerate(folds):\n",
    "    train_pairs = train_test_pairs['train']\n",
    "\n",
    "    train_data = []\n",
    "    for sub, vid in train_pairs:\n",
    "        physiology, annotations = s3.train_data(fold, sub, vid, features=features)\n",
    "        X, y = create_sliding_window_features(physiology, annotations, window_size=50)\n",
    "\n",
    "        df = X.join(y)\n",
    "\n",
    "        length = len(df)\n",
    "\n",
    "        train_length = int(length * 0.8)\n",
    "        test_length = length - train_length\n",
    "\n",
    "        train_data.append(df[:train_length])\n",
    "        df[train_length:].to_csv(os.path.join(save_prefix, f'scenario_3/test/fold_{fold}', f'sub_{sub}_vid_{vid}.csv'), index_label='time')\n",
    "\n",
    "    train_data = pd.concat(train_data, axis=0)\n",
    "    train_data.to_csv(os.path.join(save_prefix, f'scenario_3/train/fold_{fold}', 'train.csv'), index_label='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e93b4be-c583-4328-89b3-76737c5f3fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_prefix = '../data'\n",
    "save_prefix = '../splitted_data/'\n",
    "features = ['ecg', 'bvp', 'gsr', 'rsp', 'emg_zygo', 'emg_coru', 'emg_trap']\n",
    "s4 = S4()\n",
    "\n",
    "folds = s4.train_test_indices\n",
    "for fold, train_test_pairs in enumerate(folds):\n",
    "    train_pairs = train_test_pairs['train']\n",
    "\n",
    "    train_data = []\n",
    "    for sub, vid in train_pairs:\n",
    "        physiology, annotations = s4.train_data(fold, sub, vid, features=features)\n",
    "        X, y = create_sliding_window_features(physiology, annotations, window_size=50)\n",
    "\n",
    "        df = X.join(y)\n",
    "\n",
    "        length = len(df)\n",
    "\n",
    "        train_length = int(length * 0.8)\n",
    "        test_length = length - train_length\n",
    "\n",
    "        train_data.append(df[:train_length])\n",
    "        df[train_length:].to_csv(os.path.join(save_prefix, f'scenario_4/test/fold_{fold}', f'sub_{sub}_vid_{vid}.csv'), index_label='time')\n",
    "\n",
    "    train_data = pd.concat(train_data, axis=0)\n",
    "    train_data.to_csv(os.path.join(save_prefix, f'scenario_4/train/fold_{fold}', 'train.csv'), index_label='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c08626-4498-4902-a9a1-950e543dfaa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf608f-c665-4781-980a-5e387715d6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label = 'arousal'\n",
    "s1 = S1()\n",
    "subjectID = []\n",
    "videoID = []\n",
    "root_mean_squared_error = []\n",
    "mean_squared_error = []\n",
    "mean_absolute_error = []\n",
    "r2 = []\n",
    "pearsonr = []\n",
    "median_absolute_error = []\n",
    "\n",
    "leader_board_dataframe = None\n",
    "\n",
    "\n",
    "train_data = TabularDataset(os.path.join(save_prefix, f'scenario_1/train', 'train.csv'))\n",
    "train_data = train_data.drop(columns=['valence'])\n",
    "predictor = TabularPredictor(label=label, problem_type='regression', path=f'AutogluonModels/scenario_1/arousal', verbosity=0).fit(train_data, ag_args_fit={'num_gpus': 1})\n",
    "test_pairs = s1.train_test_indices['test']\n",
    "\n",
    "for sub, vid in test_pairs:\n",
    "    subjectID.append(sub)\n",
    "    videoID.append(vid)\n",
    "\n",
    "    test_data = TabularDataset(os.path.join(save_prefix, f'scenario_1/test', f'sub_{sub}_vid_{vid}.csv'))\n",
    "    y_test = test_data[label]\n",
    "    test_data_nolab = test_data.drop(columns=[label, 'valence'])\n",
    "\n",
    "    predictor = TabularPredictor.load(f'AutogluonModels/scenario_1/arousal')\n",
    "    \n",
    "    y_pred = predictor.predict(test_data_nolab)\n",
    "    perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
    "\n",
    "    root_mean_squared_error.append(perf['root_mean_squared_error'])\n",
    "    mean_squared_error.append(perf['mean_squared_error'])\n",
    "    mean_absolute_error.append(perf['mean_absolute_error'])\n",
    "    r2.append(perf['r2'])\n",
    "    pearsonr.append(perf['pearsonr'])\n",
    "    median_absolute_error.append(perf['median_absolute_error'])\n",
    "\n",
    "    subjectID_board = []\n",
    "    videoID_board = []\n",
    "\n",
    "    board = predictor.leaderboard(test_data, silent=True)\n",
    "    board_length = len(board)\n",
    "    for i in range(board_length):\n",
    "        subjectID_board.append(sub)\n",
    "        videoID_board.append(vid)\n",
    "\n",
    "    board.insert(0, 'subjectID', subjectID_board)\n",
    "    board.insert(1, 'videoID', videoID_board)\n",
    "\n",
    "    if leader_board_dataframe is None:\n",
    "        leader_board_dataframe = board\n",
    "    else:\n",
    "        leader_board_dataframe = pd.concat([leader_board_dataframe, board])\n",
    "\n",
    "evaluation_dataframe = pd.DataFrame({'subjectID': subjectID, 'videoID': videoID, 'root_mean_squared_error': root_mean_squared_error, 'mean_squared_error': mean_squared_error,\n",
    "                                'mean_absolute_error': mean_absolute_error, 'r2': r2, 'pearsonr': pearsonr, 'median_absolute_error': median_absolute_error})\n",
    "\n",
    "evaluation_dataframe.to_csv(f'AutogluonModels/scenario_1/evaluation_arousal.csv')\n",
    "leader_board_dataframe.to_csv(f'AutogluonModels/scenario_1/leaderboard_arousal.csv')\n",
    "\n",
    "print(evaluation_dataframe)\n",
    "print(leader_board_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3804e3-b7c3-4404-9456-9ac43cd9648d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Train and test on one dataset, predicting valence\n",
    "label = 'valence'\n",
    "s1 = S1()\n",
    "\n",
    "subjectID = []\n",
    "videoID = []\n",
    "root_mean_squared_error = []\n",
    "mean_squared_error = []\n",
    "mean_absolute_error = []\n",
    "r2 = []\n",
    "pearsonr = []\n",
    "median_absolute_error = []\n",
    "\n",
    "leader_board_dataframe = None\n",
    "\n",
    "train_data = TabularDataset(os.path.join(save_prefix, f'scenario_1/train', 'train.csv'))\n",
    "train_data = train_data.drop(columns=['arousal'])\n",
    "predictor = TabularPredictor(label=label, problem_type='regression', path=f'AutogluonModels/scenario_1/valence', verbosity=0).fit(train_data, ag_args_fit={'num_gpus': 1})\n",
    "test_pairs = s1.train_test_indices['test']\n",
    "\n",
    "for sub, vid in test_pairs:\n",
    "    subjectID.append(sub)\n",
    "    videoID.append(vid)\n",
    "\n",
    "    test_data = TabularDataset(os.path.join(save_prefix, f'scenario_1/test', f'sub_{sub}_vid_{vid}.csv'))\n",
    "    y_test = test_data[label]\n",
    "    test_data_nolab = test_data.drop(columns=[label, 'arousal'])\n",
    "\n",
    "    predictor = TabularPredictor.load(f'AutogluonModels/scenario_1/valence')\n",
    "    y_pred = predictor.predict(test_data_nolab)\n",
    "    perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
    "    print(perf)\n",
    "\n",
    "    root_mean_squared_error.append(perf['root_mean_squared_error'])\n",
    "    mean_squared_error.append(perf['mean_squared_error'])\n",
    "    mean_absolute_error.append(perf['mean_absolute_error'])\n",
    "    r2.append(perf['r2'])\n",
    "    pearsonr.append(perf['pearsonr'])\n",
    "    median_absolute_error.append(perf['median_absolute_error'])\n",
    "\n",
    "    subjectID_board = []\n",
    "    videoID_board = []\n",
    "\n",
    "    test_data_no_arousal = test_data.drop(columns=['arousal'])\n",
    "\n",
    "    board = predictor.leaderboard(test_data_no_arousal, silent=True)\n",
    "    board_length = len(board)\n",
    "    for i in range(board_length):\n",
    "        subjectID_board.append(sub)\n",
    "        videoID_board.append(vid)\n",
    "\n",
    "    board.insert(0, 'subjectID', subjectID_board)\n",
    "    board.insert(1, 'videoID', videoID_board)\n",
    "\n",
    "    if leader_board_dataframe is None:\n",
    "        leader_board_dataframe = board\n",
    "    else:\n",
    "        leader_board_dataframe = pd.concat([leader_board_dataframe, board])\n",
    "\n",
    "evaluation_dataframe = pd.DataFrame({'subjectID': subjectID, 'videoID': videoID, 'root_mean_squared_error': root_mean_squared_error, 'mean_squared_error': mean_squared_error,\n",
    "                                'mean_absolute_error': mean_absolute_error, 'r2': r2, 'pearsonr': pearsonr, 'median_absolute_error': median_absolute_error})\n",
    "\n",
    "evaluation_dataframe.to_csv(f'AutogluonModels/scenario_1/evaluation_valence.csv')\n",
    "leader_board_dataframe.to_csv(f'AutogluonModels/scenario_1/leaderboard_valence.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
